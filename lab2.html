
	
<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
	<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Amazon SageMaker Workshop</title>
	<meta name="viewport" content="width=device-width, initial-scale=1">

  	<!-- Facebook and Twitter integration -->
	<meta property="og:title" content=""/>
	<meta property="og:image" content=""/>
	<meta property="og:url" content=""/>
	<meta property="og:site_name" content=""/>
	<meta property="og:description" content=""/>
	<meta name="twitter:title" content="" />
	<meta name="twitter:image" content="" />
	<meta name="twitter:url" content="" />
	<meta name="twitter:card" content="" />

	<!-- Place favicon.ico and apple-touch-icon.png in the root directory -->
	<link rel="shortcut icon" href="favicon.ico">
	
	<link href='https://fonts.googleapis.com/css?family=Work+Sans:400,300,600,700' rel='stylesheet' type='text/css'>
	<link href='https://fonts.googleapis.com/css?family=Playfair+Display:400,400italic,700italic,700' rel='stylesheet' type='text/css'>
	<!-- Animate.css -->
	<link rel="stylesheet" href="css/animate.css">
	<!-- Flexslider -->
	<link rel="stylesheet" href="css/flexslider.css">
	<!-- Icomoon Icon Fonts-->
	<link rel="stylesheet" href="css/icomoon.css">
	<!-- Bootstrap  -->
	<link rel="stylesheet" href="css/bootstrap.css">

	<link rel="stylesheet" id="theme-switch" href="css/style.css">
	
	<!-- jQuery -->
	<script src="js/jquery.min.js"></script>
	<!-- jQuery Cookie -->
	<script src="js/jquery.cookie.js"></script>
	<script>
	if ( $.cookie('styleCookie') === 'style-light.css') {
		$('html, body').css('background', '#eeeeee');
	} else if ($.cookie('styleCookie') === 'style.css') {
		$('html, body').css('background', '#222222');
	}
	
	</script>
	<!-- jQuery Easing -->
	<script src="js/jquery.easing.1.3.js"></script>
	<!-- Bootstrap -->
	<script src="js/bootstrap.min.js"></script>
	<!-- Waypoints -->
	<script src="js/jquery.waypoints.min.js"></script>
	<!-- Flexslider -->
	<script src="js/jquery.flexslider-min.js"></script>

	<!-- Viewport Units Buggyfill -->
	<script src="js/viewport-units-buggyfill.js"></script>

	<!-- Googgle Map -->
	<script src="https://maps.googleapis.com/maps/api/js?key=AIzaSyCefOgb1ZWqYtj7raVSmN4PL2WkTrc-KyA&sensor=false"></script>
	<script src="js/google_map.js"></script>

	
	<!-- Main JS  -->
	<script src="js/main.js"></script>

	<!-- Modernizr JS -->
	<script src="js/modernizr-2.6.2.min.js"></script>
	<!-- FOR IE9 below -->
	<!--[if lt IE 9]>
	<script src="js/respond.min.js"></script>
	<![endif]-->

	</head>

	<body>
	
	<!-- Loader -->
	<div class="sage-loader"></div>
	
	<div id="sage-page">

			<div class="container">
				<div class="animate-box">
					<img src="images/aws_logo.png" style="width:80px; height:55px; float: left; vertical-align:middle">
					<h1 style= "font-size:55px; font-family:'Work Sans', Arial, sans-serif;">
						&nbsp;&nbsp;<span>Lab 2:</span> Customer Churn with XGBoost
				    </h1>
				</div>
			</div>
		</header>

		<div class="js-sage-waypoint sage-project-detail" id="sage-main" data-colorbg="">
			<div class="container">


				<div class="row">
					<div class="col-md-12">
						
						<div class="row row-bottom-padded-md animate-box">
							<div class="col-md-3">
								<h2 class="sage-section-heading"><span class="sage-number">S<sup>tep</sup> 1</span></h2>
							</div>
							<div class="col-md-9">
								<p class="sage-lead">Setting Up the Notebook</p>
								This first lab demonstrates how you can use the Amazon provided Built-in Xgboost algorithm. <p>
								<br>
								The example we will be going through in this lab is provided in the notebook instance that you set up in Lab 1. To access the Xgboost Customer Churn notebook, click "Open Jupyter" on the Sagemaker console, navigate to the "Sagemaker Examples" tab, and under the "Introduction to Applying Machine Learning" section, click the "Use" button next to "Xgboost Customer Churn"
								<br>

								<b>IMPORTANT:</b><br>
								Replace the S3Bucket location with a bucket you created in the Lab 1 of the workshop<br>
								<center><img src="images/lab1/pic1.png" style="width:80%"></center><p>
								<br>
								Once you have replaced the name of the bucket with your bucket's name, you can either run each cell, one at a time, by selecting the cell followed by clicking the run button, or run all cells at once. To run all cells, select Cell &#8680 Run All as shown below<br><br>
								<center><img src="images/lab1/pic2.png" style="width:40%"></center><p>
							</div>
						</div>
							
						<div class="row row-bottom-padded-md animate-box">
							<div class="col-md-3">
								<h2 class="sage-section-heading"><span class="sage-number">S<sup>tep</sup> 2</span></h2>
							</div>
							<div class="col-md-9">
								<p class="sage-lead">Data Exploration</p>
								Now scroll throw the last output and look at the data we’ll be using:<br>
								<center><img src="images/lab1/pic3.png" style="width:100%"></center><p>
								<i>In each of these next sections,  we’ll be adding new code blocks for each section.  Add new code blocks by selecting ‘Insert->Insert Cell Below’ from the top menu.  After adding the cell and code in each section, execute the block (by selecting ‘Cell->Run Cells’) and look at the results….</i>
								<p>
								Let's begin exploring the data.<p>
								This will show us the frequency/distribution of both the categories as well as the numeric features.<br>
								<pre>
# Frequency tables for each categorical feature
for column in churn.select_dtypes(include=['object']).columns:
	display(pd.crosstab(index=churn[column], columns='% observations', normalize='columns'))

# Histograms for each numeric features
display(churn.describe())
%matplotlib inline
hist = churn.hist(bins=30, sharey=True, figsize=(10, 10))</pre>
								<p>
								We can see immediately that:<br>
								<ul><li><code>State</code> appears to be quite evenly distributed.</li>
									<li><code>Phone</code> takes on too many unique values to be of any practical use. It's possible parsing out the prefix could have some value, but without more context on how these are allocated, we should avoid using it.</li>
									<li>Only 14% of customers churned, so there is some class imbalance, but nothing extreme.</li>
									<li>Most of the numeric features are surprisingly nicely distributed, with many showing bell-like gaussianity. <code>VMail Message</code> being a notable exception (and <code>Area Code</code> showing up as a feature we should convert to non-numeric).</li></ul><p>
								Based on this, we’ll drop phone number of make Area Code a non-numeric….<br>
<pre>churn = churn.drop('Phone', axis=1)
churn['Area Code'] = churn['Area Code'].astype(object)
print('dropped Phone and changed type of Area Code')</pre><p>

								Next let's look at the relationship between each of the features and our target variable.<p>
<pre>for column in churn.select_dtypes(include=['object']).columns:
    if column != 'Churn?':
        display(pd.crosstab(index=churn[column], columns=churn['Churn?'], normalize='columns'))

for column in churn.select_dtypes(exclude=['object']).columns:
    print(column)
    hist = churn[[column, 'Churn?']].hist(by='Churn?', bins=30)
    plt.show()</pre><p>
								Interestingly we see that churners appear:
								<ul><li>Fairly evenly distributed geographically</li>
									<li>More likely to have an international plan</li>
									<li>Less likely to have a voicemail plan</li>
									<li>To exhibit some bimodality in daily minutes (either higher or lower than the average for non-churners)</li>
									<li>To have a larger number of customer service calls (which makes sense as we'd expect customers who experience lots of problems may be more likely to churn)</li>
								</ul>
								In addition, we see that churners take on very similar distributions for features like Day Mins and Day Charge. That's not surprising as we'd expect minutes spent talking to correlate with charges. Let's dig deeper into the relationships between our features.
<pre>display(churn.corr())
pd.plotting.scatter_matrix(churn, figsize=(12, 12))
plt.show()</pre><p>
								We see several features that essentially have 100% correlation with one another. <p>
								<center><img src="images/lab1/pic4.png" style="width:100%"></center><p>

								XGBoost can handle this better than some other algorithms, but let’s go through the process of how to clean this up.<p>
								Let's remove one feature from each of the highly correlated pairs: Day Charge from the pair with Day Mins, Night Charge from the pair with Night Mins, Intl Charge from the pair with Intl Mins:<p>
<pre>churn = churn.drop(['Day Charge', 'Eve Charge', 'Night Charge', 'Intl Charge'], axis=1)</pre><p>
								<b>Note:</b> When running the cell above it may take a few seconds for results to display.  This is okay.  Make sure you don’t hit run twice though as you’ll get an error that looks like:
 								<center><img src="images/lab1/pic8.png" style="width:100%"></center><p>
 								Amazon SageMaker XGBoost can train on data in either a CSV or LibSVM format. For this example, we'll stick with CSV. It should:
 								<ul><li>Have the predictor variable in the first column</li>
									<li>Not have a header row</li></ul>
								But first, let's convert our categorical features into numeric features.
<pre>model_data = pd.get_dummies(churn)
model_data = pd.concat([model_data['Churn?_True.'], model_data.drop(['Churn?_False.', 'Churn?_True.'], axis=1)], axis=1)</pre><p>
								And now let's split the data into training, validation, and test sets. This will help prevent us from overfitting the model, and allow us to test the models accuracy on data it hasn't already seen.
<pre>train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7 * len(model_data)), int(0.9 * len(model_data))])
train_data.to_csv('train.csv', header=False, index=False)
validation_data.to_csv('validation.csv', header=False, index=False)</pre><p>
								Now we'll upload these files to S3.
<pre>boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')
boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')</pre>
<p>
								You should be able to see the objects now in the S3 location:
								<center><img src="images/lab1/pic5.png" style="width:80%"></center><p>
							</div>
						</div>

						<div class="row row-bottom-padded-md animate-box">
							<div class="col-md-3">
								<h2 class="sage-section-heading"><span class="sage-number">S<sup>tep</sup> 3</span></h2>
							</div>
							<div class="col-md-9">
								<p class="sage-lead">Training</p>
								Moving onto training, first we'll need to specify the locations of the XGBoost algorithm containers.
<pre>from sagemaker.amazon.amazon_estimator import get_image_uri

container = get_image_uri(boto3.Session().region_name, 'xgboost')

s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')
s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')

sess = sagemaker.Session()

xgb = sagemaker.estimator.Estimator(container,
                                    role, 
                                    train_instance_count=1, 
                                    train_instance_type='ml.m4.xlarge',
                                    output_path='s3://{}/{}/output'.format(bucket, prefix),
                                    sagemaker_session=sess)
xgb.set_hyperparameters(max_depth=5,
                        eta=0.2,
                        gamma=4,
                        min_child_weight=6,
                        subsample=0.8,
                        silent=0,
                        objective='binary:logistic',
                        num_round=100)

xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})  </pre><p>

								Running this cell triggers the training job.  You can also see the job in the SageMaker console:
 								
 								<b>Note:</b> You’ll see a lot of logs when the training job is finished running.  
							</div>
						</div>

												<div class="row row-bottom-padded-md animate-box">
							<div class="col-md-3">
								<h2 class="sage-section-heading"><span class="sage-number">S<sup>tep</sup> 4</span></h2>
							</div>
							<div class="col-md-9">
								<p class="sage-lead">Deploy Endpoint & Test</p>
								Now that we've trained the algorithm, let's create a model and deploy it to a hosted endpoint.
<pre>xgb_predictor = xgb.deploy(initial_instance_count=1,
                           instance_type='ml.m4.xlarge')</pre><p>
								Once hosted endpoint running, we can make real-time predictions from our model very easily, simply by making an http POST request. But first, we'll need to setup serializers and deserializers for passing our <code>test_data</code> NumPy arrays to the model behind the endpoint.
<pre>xgb_predictor.content_type = 'text/csv'
xgb_predictor.serializer = csv_serializer
xgb_predictor.deserializer = None</pre><p>
								Now, we'll use a simple function to:
								<ol><li>Loop over our test dataset</li>
									<li>Split it into mini-batches of rows</li>
									<li>Convert those mini-batchs to CSV string payloads</li>
									<li>Retrieve mini-batch predictions by invoking the XGBoost endpoint</li>
									<li>Collect predictions and convert from the CSV output our model provides into a NumPy array</li>
								</ol>
<pre>def predict(data, rows=500):
    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))
    predictions = ''
    for array in split_array:
        predictions = ','.join([predictions, xgb_predictor.predict(array).decode('utf-8')])

    return np.fromstring(predictions[1:], sep=',')

predictions = predict(test_data.as_matrix()[:, 1:])</pre><p>
								There are many ways to compare the performance of a machine learning model, but let's start by simply by comparing actual to predicted values. In this case, we're simply predicting whether the customer churned (1) or not (0), which produces a simple confusion matrix.
<pre>pd.crosstab(index=test_data.iloc[:, 0], columns=np.round(predictions), rownames=['actual'], colnames=['predictions'])</pre><p>
								<i>Note, due to randomized elements of the algorithm, you results may differ slightly.</i><p>
								Of the 48 churners, we've correctly predicted 39 of them (true positives). And, we incorrectly predicted 4 customers would churn who then ended up not doing so (false positives). There are also 9 customers who ended up churning, that we predicted would not (false negatives).<p>
								
								An important point here is that because of the <code>np.round()</code> function above we are using a simple threshold (or cutoff) of 0.5. Our predictions from <code>xgboost</code> come out as continuous values between 0 and 1 and we force them into the binary classes that we began with. However, because a customer that churns is expected to cost the company more than proactively trying to retain a customer who we think might churn, we should consider adjusting this cutoff. That will almost certainly increase the number of false positives, but it can also be expected to increase the number of true positives and reduce the number of false negatives.<p>
								
								To get a rough intuition here, let's look at the continuous values of our predictions.
<pre>plt.hist(predictions)
plt.show()</pre><p>
								The continuous valued predictions coming from our model tend to skew toward 0 or 1, but there is sufficient mass between 0.1 and 0.9 that adjusting the cutoff should indeed shift a number of customers' predictions. For example...
<pre>pd.crosstab(index=test_data.iloc[:, 0], columns=np.where(predictions > 0.3, 1, 0))</pre><p>
								We can see that changing the cutoff from 0.5 to 0.3 results in 1 more true positives, 3 more false positives, and 1 fewer false negatives. The numbers are small overall here, but that's 6-10% of customers overall that are shifting because of a change to the cutoff. Was this the right decision? We may end up retaining 3 extra customers, but we also unnecessarily incentivized 5 more customers who would have stayed. Determining optimal cutoffs is a key step in properly applying machine learning in a real-world setting. Let's discuss this more broadly and then apply a specific, hypothetical solution for our current problem.<p>
								
								<p class="sage-lead">Relative cost of errors</p>
								Any practical binary classification problem is likely to produce a similarly sensitive cutoff. That by itself isn’t a problem. After all, if the scores for two classes are really easy to separate, the problem probably isn’t very hard to begin with and might even be solvable with simple rules instead of ML.<p>
								
								More important, if I put an ML model into production, there are costs associated with the model erroneously assigning false positives and false negatives. I also need to look at similar costs associated with correct predictions of true positives and true negatives. Because the choice of the cutoff affects all four of these statistics, I need to consider the relative costs to the business for each of these four outcomes for each prediction.<p>
								
								<h4>Assigning costs</h4>
								What are the costs for our problem of mobile operator churn? The costs, of course, depend on the specific actions that the business takes. Let's make some assumptions here.<p>

								First, assign the true negatives the cost of $0. Our model essentially correctly identified a happy customer in this case, and we don’t need to do anything.<p>

								False negatives are the most problematic, because they incorrectly predict that a churning customer will stay. We lose the customer and will have to pay all the costs of acquiring a replacement customer, including foregone revenue, advertising costs, administrative costs, point of sale costs, and likely a phone hardware subsidy. A quick search on the Internet reveals that such costs typically run in the hundreds of dollars so, for the purposes of this example, let's assume $500. This is the cost of false negatives.<p>

								Finally, for customers that our model identifies as churning, let's assume a retention incentive in the amount of $100. If my provider offered me such a concession, I’d certainly think twice before leaving. This is the cost of both true positive and false positive outcomes. In the case of false positives (the customer is happy, but the model mistakenly predicted churn), we will “waste” the $100 concession. We probably could have spent that $100 more effectively, but it's possible we increased the loyalty of an already loyal customer, so that’s not so bad.<p>

								<h4>Finding the optimal cutoff</h4>
								It’s clear that false negatives are substantially more costly than false positives. Instead of optimizing for error based on the number of customers, we should be minimizing a cost function that looks like this:<br>
								<code>$500 * FN(C) + $0 * TN(C) + $100 * FP(C) + $100 * TP(C)</code><br>
								FN(C) means that the false negative percentage is a function of the cutoff, C, and similar for TN, FP, and TP. We need to find the cutoff, C, where the result of the expression is smallest.<p>

								A straightforward way to do this, is to simply run a simulation over a large number of possible cutoffs. We test 100 possible values in the for loop below.
<pre>cutoffs = np.arange(0.01, 1, 0.01)
costs = []
for c in cutoffs:
    costs.append(np.sum(np.sum(np.array([[0, 100], [500, 100]]) * 
                               pd.crosstab(index=test_data.iloc[:, 0], 
                                           columns=np.where(predictions > c, 1, 0)))))

costs = np.array(costs)
plt.plot(cutoffs, costs)
plt.show()
print('Cost is minimized near a cutoff of:', cutoffs[np.argmin(costs)], 'for a cost of:', np.min(costs))</pre><p>
								The above chart shows how picking a threshold too low results in costs skyrocketing as all customers are given a retention incentive. Meanwhile, setting the threshold too high results in too many lost customers, which ultimately grows to be nearly as costly. The overall cost can be minimized at $8400 by setting the cutoff to 0.46, which is substantially better than the $20k+ I would expect to lose by not taking any action.

							</div>
						</div>

						<div class="row row-bottom-padded-md animate-box">
							<div class="col-md-3">
								<h2 class="sage-section-heading"><span class="sage-number">S<sup>tep</sup> 5</span></h2>
							</div>
							<div class="col-md-9">
								<p class="sage-lead">Clean Up</p>
								Please run the cell below. This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on.
<pre>sagemaker.Session().delete_endpoint(xgb_predictor.endpoint)</pre>
 							<p><p>
								<h2><center>End Of Lab, <a href="index.html" class="transition">Click to Return Home</a></center></h2>	
							</div>
						</div>
						</div>
					</div>
				</div> 

	

			</div>
		</div>

		<footer id="sage-footer" class="js-sage-waypoint">
			<div class="container">
				<div class="row">
					<div class="col-md-12 text-center">
						<p><small>&copy; 2018, Amazon Web Services, Inc. or its affiliates. All rights reserved.</small> </p>
						<ul class="sage-social">
							<li>
								<a href="https://twitter.com/awscloud"><i class="icon-twitter"></i></a>
							</li>
							<li>
								<a href="https://www.instagram.com/amazonwebservices/"><i class="icon-instagram"></i></a>
							</li>
							<li>
								<a href="https://aws.amazon.com/blogs/aws/"><i class="icon-home"></i></a>
							</li>
						</ul>
					</div>
				</div>
			</div>
		</footer>

	</div>



	</body>
</html>

